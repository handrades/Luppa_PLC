# Story 5.3: Bulk Import/Export Operations

## Status

Approved

## Story

**As a** data administrator,
**I want** to import and export equipment data in bulk,
**so that** I can efficiently manage large datasets and migrations.

## Acceptance Criteria

1. **CSV Template Downloadable**: CSV template downloadable with all required fields
2. **Drag-and-Drop CSV Upload**: Drag-and-drop CSV upload with progress indicator
3. **Import Preview**: Import preview shows first 10 rows with validation status
4. **Validation Errors**: Validation errors displayed per row with clear messages
5. **Duplicate Detection**: Duplicate detection based on IP address with merge options
6. **Export with Filters**: Export includes filters to download specific subsets
7. **Background Processing**: Background processing for files over 1000 rows
8. **Import History**: Import history log with rollback capability

## Tasks / Subtasks

### Backend Import Service Implementation

- [x] **Create ImportExportService Module** (AC: 1, 2, 3, 4, 5, 8)
  - [x] Create `/apps/api/src/services/ImportExportService.ts` with TypeScript interfaces
  - [x] Implement `importPLCs(file: Buffer, options: ImportOptions): Promise<ImportResult>` method
  - [x] Implement `validateCSV(file: Buffer): Promise<ValidationResult>` method with row-by-row validation
  - [x] Implement `generateTemplate(): Promise<Buffer>` method for CSV template generation
  - [x] Add duplicate detection logic using IP address with configurable merge strategies
  - [x] Implement transaction management with atomic import operations and rollback capability
  - [x] Create auto-creation logic for missing sites/cells/equipment when enabled
  - [x] Add comprehensive error handling with detailed row-level error reporting

- [x] **Implement CSV Processing Logic** (AC: 2, 3, 4, 5)
  - [x] Add csv-parse library integration with streaming for large files
  - [x] Implement header validation against expected schema
  - [x] Create row-by-row validation with specific error messages per field
  - [x] Add IP address format validation and uniqueness checking
  - [x] Implement hierarchy validation (site → cell → equipment → plc relationships)
  - [x] Create data type validation for equipment types and tag data types
  - [x] Add sanitization for special characters in CSV fields

- [x] **Implement Background Processing** (AC: 7, 8)
  - [x] Create job queue system for large file processing (1000+ rows)
  - [x] Implement import progress tracking with status updates
  - [x] Add import history persistence with metadata (user, timestamp, file info)
  - [x] Implement rollback mechanism for failed imports
  - [x] Create import status endpoints for progress monitoring
  - [x] Add cleanup logic for temporary files and failed imports

### Backend Export Service Implementation

- [x] **Create Export Functionality** (AC: 6)
  - [x] Implement `exportPLCs(filters: PLCFilters, format: ExportFormat): Promise<Buffer>` method
  - [x] Add streaming CSV generation for large datasets using csv-stringify
  - [x] Implement filter processing to apply site, equipment type, and date range filters
  - [x] Include full hierarchy information (site_name, cell_name, equipment_name) in exports
  - [x] Add custom field selection for export (includeHierarchy, includeTags options)
  - [x] Implement proper CSV escaping for special characters and multi-line fields
  - [x] Add export metadata (timestamp, user, filter criteria) to file headers

### API Endpoints Implementation

- [x] **Create Import/Export Routes** (AC: 1, 2, 6, 7, 8)
  - [x] Create `/apps/api/src/routes/import-export.ts` with all endpoints
  - [x] Implement `POST /api/v1/import/plcs` with multipart/form-data support
  - [x] Implement `POST /api/v1/export/plcs` with JSON request body for filters
  - [x] Implement `GET /api/v1/import/template` for CSV template download
  - [x] Implement `GET /api/v1/import/history` for import history listing
  - [x] Implement `POST /api/v1/import/{importId}/rollback` for rollback operations
  - [x] Add Joi validation schemas for all request parameters and options
  - [x] Implement proper error responses with detailed validation messages

- [x] **Add Middleware and Security** (AC: 2, 7, 8)
  - [x] Implement file upload middleware with size limits and type validation
  - [x] Add authentication middleware for all import/export endpoints
  - [x] Implement rate limiting for bulk operations to prevent abuse
  - [x] Add audit logging for all import/export operations
  - [x] Implement request ID tracking for background job correlation
  - [x] Add virus scanning for uploaded files (if available in air-gapped environment)

### Frontend Import Components

- [ ] **Create File Upload Component** (AC: 2, 3)
  - [ ] Create `/apps/web/src/components/import/FileUpload.tsx` with drag-and-drop support
  - [ ] Implement Material-UI DropzoneArea with file validation
  - [ ] Add progress indicator with percentage and estimated time remaining
  - [ ] Implement file size and type validation with user feedback
  - [ ] Add multiple file upload support with batch processing
  - [ ] Create upload status display with success/error states
  - [ ] Implement cancel upload functionality for large files

- [ ] **Create Import Preview Component** (AC: 3, 4)
  - [ ] Create `/apps/web/src/components/import/ImportPreview.tsx` with data table
  - [ ] Display first 10 rows with validation status indicators
  - [ ] Implement row-level error display with expandable details
  - [ ] Add column mapping interface for flexible CSV formats
  - [ ] Create validation summary with error counts and categories
  - [ ] Implement fix suggestions for common validation errors
  - [ ] Add option to skip invalid rows or fix them inline

- [ ] **Create Import Options Component** (AC: 5, 7)
  - [ ] Create `/apps/web/src/components/import/ImportOptions.tsx` with configuration
  - [ ] Add toggle for auto-creation of missing hierarchy entities
  - [ ] Implement duplicate handling options (skip, overwrite, merge, create new)
  - [ ] Add background processing threshold configuration
  - [ ] Create import mode selection (validate only, import immediately)
  - [ ] Implement dry-run option for testing imports without persistence
  - [ ] Add notification preferences for import completion

### Frontend Export Components

- [ ] **Create Export Configuration Component** (AC: 6)
  - [ ] Create `/apps/web/src/components/export/ExportConfig.tsx` with filter interface
  - [ ] Implement filter selection using existing DataGrid filter components
  - [ ] Add field selection interface for customizing export columns
  - [ ] Create export format selection (CSV, with future Excel/PDF support)
  - [ ] Implement preview of export results with estimated record count
  - [ ] Add export naming options with timestamp and filter description
  - [ ] Create scheduled export functionality for recurring reports

### State Management and Services

- [ ] **Create Import/Export Store** (AC: 2, 3, 7, 8)
  - [ ] Create `/apps/web/src/stores/importExport.store.ts` using Zustand patterns
  - [ ] Manage file upload state with progress tracking
  - [ ] Store import history with local caching
  - [ ] Handle background job status polling
  - [ ] Manage export configuration and recent exports
  - [ ] Implement import/export preferences persistence
  - [ ] Add error state management with retry mechanisms

- [ ] **Create Import/Export Service** (AC: 1, 6, 7, 8)
  - [ ] Create `/apps/web/src/services/importExport.service.ts` using axios patterns
  - [ ] Implement file upload with progress tracking using FormData
  - [ ] Add export request handling with download blob creation
  - [ ] Implement polling service for background job status
  - [ ] Add template download functionality
  - [ ] Create import history fetching with pagination
  - [ ] Implement rollback operation calls with confirmation

### Integration and Performance

- [x] **Database Integration** (AC: 5, 8)
  - [x] Ensure proper foreign key constraint handling during imports
  - [x] Implement optimistic locking for concurrent import operations
  - [x] Add database connection pooling configuration for bulk operations
  - [x] Create database migration for import history table
  - [x] Implement cascade deletion handling for rollback operations
  - [x] Add database performance monitoring for large imports

- [x] **Performance Optimization** (AC: 7)
  - [x] Implement streaming for files over specified threshold (1000 rows)
  - [x] Add memory management for large file processing
  - [x] Implement batch processing for database operations
  - [x] Add caching for frequently accessed hierarchy data during imports
  - [x] Create connection pooling optimization for concurrent operations
  - [x] Implement file compression for export downloads

### Testing Implementation

- [x] **Backend Testing**
  - [x] Unit tests for ImportExportService with comprehensive CSV processing scenarios
  - [x] Integration tests for import/export API endpoints with real CSV files
  - [x] Performance tests with large datasets (1000+, 5000+, 10000+ records)
  - [x] Error handling tests for malformed CSV files and invalid data
  - [x] Transaction rollback tests for failed import scenarios
  - [x] Concurrent operation tests for multiple simultaneous imports/exports

- [ ] **Frontend Testing**
  - [ ] Component tests for file upload with drag-and-drop simulation
  - [ ] Integration tests for complete import workflow using React Testing Library
  - [ ] E2E tests for full import/export cycles using Playwright
  - [ ] File handling tests for various CSV formats and sizes
  - [ ] User interaction tests for error handling and retry scenarios
  - [ ] Accessibility tests for file upload and table components

## Dev Notes

### Import/Export Service Architecture

[Source: architecture/components.md#import/export-service]

**Service Responsibility**: Bulk data operations, CSV parsing/generation, hierarchy validation during import, auto-creation of missing entities

**Key Service Interfaces**:

```typescript
interface ImportOptions {
  createMissing: boolean;
  duplicateHandling: "skip" | "overwrite" | "merge";
  backgroundThreshold: number;
  validateOnly: boolean;
}

interface ImportResult {
  success: boolean;
  importId: string;
  totalRows: number;
  successfulRows: number;
  failedRows: number;
  errors: ValidationError[];
  createdEntities: CreatedEntitySummary;
  isBackground: boolean;
}

interface ValidationResult {
  isValid: boolean;
  headerErrors: string[];
  rowErrors: RowValidationError[];
  preview: any[];
}
```

**Dependencies**: All entity services (Site, Cell, Equipment, PLC), csv-parse, csv-stringify libraries
**Technology Stack**: TypeScript, streaming for large files, transaction support for atomic imports

### Bulk Import Workflow Details

[Source: architecture/core-workflows.md#bulk-plc-import-flow]

**Import Process Flow**:

1. **File Upload**: Multipart form data with progress tracking
2. **CSV Parsing**: Stream-based parsing with header validation
3. **Row Validation**: Individual row validation with detailed error reporting
4. **Transaction Management**: Atomic operations with rollback capability
5. **Auto-Creation Logic**: Optional creation of missing hierarchy entities
6. **Audit Logging**: Comprehensive logging of all import operations

**Critical Implementation Details**:

- Use PostgreSQL transactions for atomic imports
- Implement duplicate detection based on IP address uniqueness constraint
- Support createMissing flag for auto-creation of sites/cells/equipment
- Provide detailed validation errors with row numbers and field names
- Include rollback mechanism for failed imports

### Export with Filters Workflow

[Source: architecture/core-workflows.md#export-with-filters-flow]

**Export Process Flow**:

1. **Filter Configuration**: Apply site, equipment type, date range filters
2. **Query Execution**: Generate filtered query using existing PLC search patterns
3. **CSV Generation**: Stream-based CSV generation with hierarchy columns
4. **File Download**: Browser download with proper Content-Type headers

**Export Features**:

- Include hierarchy information (site_name, cell_name, equipment_name)
- Support custom field selection (includeHierarchy, includeTags options)
- Add export metadata in file headers (timestamp, user, filters applied)
- Implement streaming for large exports to prevent memory issues

### API Specification Details

[Source: architecture/api-specification.md]

**Import Endpoint**:

```yaml
POST /api/v1/import/plcs
Content-Type: multipart/form-data
Body:
  file: binary (CSV file)
  createMissing: boolean (default: false)
  duplicateHandling: string (skip|overwrite|merge)
  backgroundThreshold: number (default: 1000)
```

**Export Endpoint**:

```yaml
POST /api/v1/export/plcs
Content-Type: application/json
Body:
  filters: PLCFilters object
  includeHierarchy: boolean (default: true)
  includeTags: boolean (default: false)
  format: string (default: 'csv')
```

### Database Schema Considerations

[Source: architecture/database-schema.md]

**Relevant Tables for Import/Export**:

- `sites`: UUID primary key, unique name constraint
- `cells`: Foreign key to sites, unique (site_id, line_number) constraint
- `equipment`: Foreign key to cells, equipment_type enum validation
- `plcs`: Foreign key to equipment, unique tag_id and ip_address constraints
- `audit_logs`: Comprehensive audit trail for all import operations

**Key Constraints**:

- All tables have CASCADE DELETE for proper hierarchy cleanup
- IP address uniqueness constraint on plcs table
- Equipment type enum validation
- Created_by/updated_by foreign keys to users table for audit trail

**Import History Table** (to be created):

```sql
CREATE TABLE import_history (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES users(id),
    filename VARCHAR(255) NOT NULL,
    total_rows INTEGER NOT NULL,
    successful_rows INTEGER NOT NULL,
    failed_rows INTEGER NOT NULL,
    options JSONB NOT NULL,
    errors JSONB,
    created_entities JSONB,
    status VARCHAR(50) NOT NULL,
    started_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP,
    CONSTRAINT fk_import_history_user FOREIGN KEY (user_id) REFERENCES users(id)
);
```

### Frontend Component Architecture

[Source: architecture/frontend-architecture.md]

**Component Organization**:

- Import components: `/apps/web/src/components/import/`
- Export components: `/apps/web/src/components/export/`
- Shared utilities: `/apps/web/src/utils/csv.utils.ts`

**State Management Pattern**:

```typescript
interface ImportExportStore {
  // Import state
  uploadProgress: number;
  importStatus: ImportStatus;
  previewData: any[];
  validationErrors: ValidationError[];

  // Export state
  exportFilters: PLCFilters;
  exportProgress: number;

  // History state
  importHistory: ImportHistoryItem[];

  // Actions
  uploadFile: (file: File, options: ImportOptions) => Promise<void>;
  exportData: (filters: PLCFilters) => Promise<void>;
  getImportHistory: () => Promise<void>;
  rollbackImport: (importId: string) => Promise<void>;
}
```

**Material-UI Integration**:

- Use existing industrial theme and component patterns
- Follow DataGrid patterns for table displays
- Implement consistent error handling and loading states
- Use established form validation patterns with React Hook Form

### CSV Processing Technology Stack

[Source: architecture/tech-stack.md]

**Backend CSV Processing**:

- **csv-parse**: Stream-based CSV parsing with automatic type detection
- **csv-stringify**: CSV generation with proper escaping and formatting
- **Node.js Streams**: Memory-efficient processing of large files
- **TypeScript 5.8.3**: Full type safety for CSV data structures

**Frontend File Handling**:

- **Material-UI DropzoneArea**: Drag-and-drop file upload component
- **Axios**: File upload with progress tracking using FormData
- **Browser File API**: Client-side file validation and reading

### Performance and Memory Management

**Large File Handling**:

- Stream-based processing to prevent memory overflow
- Background job processing for files over 1000 rows
- Connection pooling configuration for database operations
- File size limits and validation (recommend 10MB max for air-gapped environments)

**Memory Optimization**:

- Process CSV files in chunks rather than loading entirely into memory
- Use database streaming queries for large exports
- Implement garbage collection between batch operations
- Monitor memory usage during bulk operations

### Error Handling and Validation

**CSV Validation Rules**:

- Required fields: site_name, cell_name, equipment_name, tag_id, description, make, model
- Optional fields: ip_address, firmware_version, equipment_type
- Data type validation for IP addresses (IPv4/IPv6 format)
- Enum validation for equipment_type
- Uniqueness validation for tag_id and ip_address

**Error Response Format**:

```typescript
interface ValidationError {
  row: number;
  field: string;
  value: any;
  error: string;
  severity: "error" | "warning";
}

interface ImportError {
  type: "validation" | "database" | "system";
  message: string;
  details: ValidationError[];
}
```

### Security and Audit Considerations

**File Security**:

- Validate file extensions and MIME types
- Implement file size limits to prevent DoS attacks
- Sanitize CSV content to prevent CSV injection attacks
- Scan uploaded files for malicious content (if available)

**Audit Trail Requirements**:

- Log all import operations with user context
- Track all entity creation/modification during imports
- Maintain rollback capability with full audit trail
- Record file metadata (name, size, checksum) for compliance

### Integration with Existing Systems

**Story 5.1 and 5.2 Integration**:

- Reuse existing filter components from advanced filtering system
- Leverage search functionality for duplicate detection
- Integrate with existing audit logging patterns
- Use established error handling and loading state patterns

**Equipment Management Integration**:

- Connect with existing equipment store and services (Stories 4.2-4.5)
- Use existing hierarchy validation patterns
- Follow established CRUD operation patterns
- Maintain consistency with existing API response formats

## Testing

### Testing Standards from Architecture

[Source: architecture/development-workflow-testing-strategy.md]

**Test File Locations**:

- Backend tests: `/apps/api/src/__tests__/services/ImportExportService.test.ts`
- Backend integration: `/apps/api/src/__tests__/routes/import-export.test.ts`
- Frontend component tests: `/apps/web/src/components/import/__tests__/`, `/apps/web/src/components/export/__tests__/`
- Frontend service tests: `/apps/web/src/services/__tests__/importExport.service.test.ts`
- E2E tests: `/apps/web/src/__tests__/e2e/import-export.e2e.test.ts`

**Testing Frameworks and Patterns**:

- Backend: Jest with Supertest for API endpoint testing, following patterns from existing test files
- Frontend: Jest with React Testing Library for component testing
- E2E: Playwright for end-to-end workflow testing
- File testing: Create test CSV files with various scenarios (valid, invalid, large datasets)

**CSV Testing Scenarios**:

- Valid CSV with all required fields
- CSV with missing required fields
- CSV with invalid data types (invalid IP addresses, wrong enums)
- CSV with duplicate records (same tag_id or ip_address)
- CSV with special characters and Unicode content
- Large CSV files (1000+, 5000+ rows) for performance testing
- Malformed CSV files (missing headers, inconsistent columns)

**Integration Testing Requirements**:

- Test complete import workflow from upload to database persistence
- Test rollback functionality with database state verification
- Test export functionality with various filter combinations
- Test concurrent import operations
- Test background job processing and status tracking

**Performance Testing**:

- Measure import performance with datasets of varying sizes
- Validate memory usage during large file processing
- Test concurrent user operations
- Verify database performance during bulk operations
- Monitor file processing times and set performance benchmarks

## Change Log

| Date       | Version | Description                                                                         | Author       |
| ---------- | ------- | ----------------------------------------------------------------------------------- | ------------ |
| 2025-01-24 | 1.0     | Initial story creation with comprehensive requirements and technical specifications | Scrum Master |

## Dev Agent Record

### Agent Model Used

Claude Opus 4.1 - Full Stack Development Agent

### Implementation Started

**Date**: 2025-01-24
**Story Status**: In Progress - Backend Implementation Phase

### Debug Log References

Starting implementation with backend ImportExportService module creation.

### Completion Notes

Starting with backend services and API layer before frontend components.

### File List

**Backend Implementation:**

- `/apps/api/src/services/ImportExportService.ts` - Core import/export service with CSV processing
- `/apps/api/src/entities/ImportHistory.ts` - Import history entity for tracking operations
- `/apps/api/src/routes/import-export.ts` - REST API endpoints for import/export operations
- `/apps/api/src/errors/DatabaseError.ts` - Database error handling class
- `/apps/api/src/migrations/1755714547340-AddImportHistoryTable.ts` - Database migration for import history
- `/apps/api/src/app.ts` - Updated to include import/export routes
- `/apps/api/package.json` - Added CSV processing dependencies (csv-parse, csv-stringify, multer)

**Tests:**

- `/apps/api/src/__tests__/services/ImportExportService.test.ts` - Unit tests for service logic
- `/apps/api/src/__tests__/routes/import-export.test.ts` - Integration tests for API endpoints

## QA Results

_This section will be populated by the QA agent during testing and validation_
