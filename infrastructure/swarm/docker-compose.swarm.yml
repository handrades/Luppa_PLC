# Docker Swarm Deployment Configuration for Luppa PLC Inventory System
# Production-ready orchestration with high availability and rolling updates

version: '3.8'

networks:
  luppa-prod:
    driver: overlay
    name: luppa-prod-network
    attachable: false
    encrypted: true
    driver_opts:
      encrypted: ''
  luppa-monitoring:
    driver: overlay
    name: luppa-monitoring-network
    attachable: false
    encrypted: true

volumes:
  postgres-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/luppa/data/postgres
  redis-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/luppa/data/redis
  grafana-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/luppa/data/grafana
  prometheus-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/luppa/data/prometheus
  ssl-certs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/luppa/ssl/certs

configs:
  nginx-config:
    file: ../../infrastructure/nginx/nginx.prod.conf
  prometheus-config:
    file: ../../infrastructure/monitoring/prometheus/prometheus.yml
  grafana-config:
    file: ../../infrastructure/monitoring/grafana/grafana.ini

secrets:
  postgres-password:
    external: true
  redis-password:
    external: true
  jwt-secret:
    external: true
  ssl-cert:
    external: true
  ssl-key:
    external: true

services:
  postgres:
    image: postgres:17
    hostname: postgres
    networks:
      - luppa-prod
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-luppa_prod}
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres-password
      POSTGRES_HOST_AUTH_METHOD: ${POSTGRES_HOST_AUTH_METHOD:-scram-sha-256}
      POSTGRES_INITDB_ARGS: '--auth-host=scram-sha-256 --auth-local=scram-sha-256'
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - type: bind
        source: ../../infrastructure/postgres/initdb
        target: /docker-entrypoint-initdb.d
        read_only: true
    secrets:
      - postgres-password
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-luppa_prod}']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
          - node.labels.postgres == true
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
        reservations:
          memory: 1G
          cpus: '0.5'
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
        monitor: 60s
        order: stop-first

  pgbouncer:
    image: pgbouncer/pgbouncer:1.23
    hostname: pgbouncer
    networks:
      - luppa-prod
    environment:
      DATABASES_HOST: postgres
      DATABASES_PORT: 5432
      DATABASES_USER: ${POSTGRES_USER:-postgres}
      DATABASES_PASSWORD_FILE: /run/secrets/postgres-password
      DATABASES_DBNAME: ${POSTGRES_DB:-luppa_prod}
      POOL_MODE: transaction
      MAX_CLIENT_CONN: 100
      DEFAULT_POOL_SIZE: 25
      MIN_POOL_SIZE: 5
      RESERVE_POOL_SIZE: 5
      SERVER_RESET_QUERY: DISCARD ALL
      IGNORE_STARTUP_PARAMETERS: extra_float_digits
      LOG_CONNECTIONS: 1
      LOG_DISCONNECTIONS: 1
    secrets:
      - postgres-password
    healthcheck:
      test: ['CMD-SHELL', 'nc -z localhost 5432 || exit 1']
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    deploy:
      replicas: 2
      placement:
        max_replicas_per_node: 1
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.1'
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        order: start-first
    depends_on:
      - postgres

  redis:
    image: redis:8-alpine
    hostname: redis
    networks:
      - luppa-prod
    command: >
      redis-server
      --requirepass_file /run/secrets/redis-password
      --appendonly yes
      --appendfsync everysec
      --auto-aof-rewrite-percentage 100
      --auto-aof-rewrite-min-size 64mb
      --maxmemory ${REDIS_MAX_MEMORY:-1gb}
      --maxmemory-policy ${REDIS_EVICTION_POLICY:-allkeys-lru}
      --save 900 1
      --save 300 10
      --save 60 10000
      --tcp-keepalive 300
      --timeout 0
      --bind 0.0.0.0
      --protected-mode yes
    volumes:
      - redis-data:/data
    secrets:
      - redis-password
    healthcheck:
      test:
        ['CMD-SHELL', 'redis-cli --no-auth-warning -a "$(cat /run/secrets/redis-password)" ping']
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.redis == true
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.25'
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
        monitor: 60s
        order: stop-first

  api:
    image: ${DOCKER_REGISTRY:-}luppa-api:${API_VERSION:-latest}
    hostname: api
    networks:
      - luppa-prod
      - luppa-monitoring
    environment:
      NODE_ENV: production
      PORT: ${API_PORT:-3000}
      HOST: 0.0.0.0
      LOG_LEVEL: ${LOG_LEVEL:-info}
      POSTGRES_HOST: pgbouncer
      POSTGRES_PORT: 5432
      POSTGRES_DB: ${POSTGRES_DB:-luppa_prod}
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_SSL: ${POSTGRES_SSL:-false}
      REDIS_HOST: redis
      REDIS_PORT: 6379
      CORS_ORIGIN: ${CORS_ORIGIN:-https://localhost}
      SESSION_SECRET_FILE: /run/secrets/jwt-secret
      METRICS_PORT: 9090
      METRICS_PATH: /metrics
    secrets:
      - jwt-secret
    healthcheck:
      test:
        [
          'CMD',
          'wget',
          '--no-verbose',
          '--tries=1',
          '--spider',
          'http://localhost:${API_PORT:-3000}/health',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      replicas: 2
      placement:
        max_replicas_per_node: 1
        preferences:
          - spread: node.labels.zone
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        monitor: 60s
        order: start-first
      rollback_config:
        parallelism: 1
        delay: 10s
        monitor: 60s
        order: stop-first
    depends_on:
      - pgbouncer
      - redis

  web:
    image: ${DOCKER_REGISTRY:-}luppa-web:${WEB_VERSION:-latest}
    hostname: web
    networks:
      - luppa-prod
    environment:
      NODE_ENV: production
      VITE_API_URL: ${VITE_API_URL:-https://localhost/api/v1}
      VITE_APP_VERSION: ${VITE_APP_VERSION:-latest}
      VITE_LOG_LEVEL: ${VITE_LOG_LEVEL:-warn}
    healthcheck:
      test: ['CMD', 'wget', '--no-verbose', '--tries=1', '--spider', 'http://localhost:80']
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      replicas: 2
      placement:
        max_replicas_per_node: 1
        preferences:
          - spread: node.labels.zone
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        monitor: 30s
        order: start-first

  nginx:
    image: nginx:1.27-alpine
    hostname: nginx
    networks:
      - luppa-prod
    ports:
      - target: 80
        published: 80
        protocol: tcp
        mode: ingress
      - target: 443
        published: 443
        protocol: tcp
        mode: ingress
    configs:
      - source: nginx-config
        target: /etc/nginx/nginx.conf
        mode: 0444
    secrets:
      - source: ssl-cert
        target: /etc/ssl/certs/server.crt
        mode: 0444
      - source: ssl-key
        target: /etc/ssl/private/server.key
        mode: 0400
    healthcheck:
      test: ['CMD', 'wget', '--no-verbose', '--tries=1', '--spider', 'http://localhost/health']
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.25'
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
        monitor: 60s
        order: start-first
    depends_on:
      - api
      - web

  prometheus:
    image: prom/prometheus:v3.0.1
    hostname: prometheus
    networks:
      - luppa-monitoring
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--storage.tsdb.retention.size=10GB'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
      - '--log.level=info'
      - '--web.external-url=http://localhost/prometheus'
      - '--web.route-prefix=/'
    volumes:
      - prometheus-data:/prometheus
      - type: bind
        source: ../../infrastructure/monitoring/prometheus/rules
        target: /etc/prometheus/rules
        read_only: true
    configs:
      - source: prometheus-config
        target: /etc/prometheus/prometheus.yml
        mode: 0444
    healthcheck:
      test:
        ['CMD', 'wget', '--no-verbose', '--tries=1', '--spider', 'http://localhost:9090/-/healthy']
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
          - node.labels.monitoring == true
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 120s
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
        monitor: 60s
        order: stop-first

  grafana:
    image: grafana/grafana:11.4.0
    hostname: grafana
    networks:
      - luppa-monitoring
      - luppa-prod
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin}
      GF_SECURITY_SECRET_KEY: ${GRAFANA_SECRET_KEY}
      GF_USERS_ALLOW_SIGN_UP: false
      GF_INSTALL_PLUGINS: ${GRAFANA_INSTALL_PLUGINS:-}
      GF_SECURITY_DISABLE_GRAVATAR: true
      GF_ANALYTICS_REPORTING_ENABLED: false
      GF_ANALYTICS_CHECK_FOR_UPDATES: false
      GF_LOG_LEVEL: info
      GF_SERVER_DOMAIN: ${GRAFANA_DOMAIN:-localhost}
      GF_SERVER_ROOT_URL: https://${GRAFANA_DOMAIN:-localhost}/monitoring/
      GF_SERVER_SERVE_FROM_SUB_PATH: true
    volumes:
      - grafana-data:/var/lib/grafana
      - type: bind
        source: ../../infrastructure/monitoring/grafana/dashboards
        target: /etc/grafana/provisioning/dashboards
        read_only: true
      - type: bind
        source: ../../infrastructure/monitoring/grafana/datasources
        target: /etc/grafana/provisioning/datasources
        read_only: true
    configs:
      - source: grafana-config
        target: /etc/grafana/grafana.ini
        mode: 0444
    healthcheck:
      test:
        ['CMD', 'wget', '--no-verbose', '--tries=1', '--spider', 'http://localhost:3000/api/health']
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
          - node.labels.monitoring == true
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 120s
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
        monitor: 60s
        order: start-first
    depends_on:
      - prometheus
